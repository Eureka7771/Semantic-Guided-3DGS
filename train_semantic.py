#!/usr/bin/env python3
"""
è¯­ä¹‰å¢å¼ºçš„3Dé«˜æ–¯è®­ç»ƒè„šæœ¬ - å…¨ç¨‹0.5åˆ†è¾¨ç‡ç‰ˆæœ¬
"""

import os
import torch
import torch.nn.functional as F
import numpy as np
from random import randint
from utils.loss_utils import l1_loss, ssim
from gaussian_renderer import render, network_gui
import sys
from scene import Scene, GaussianModel
from utils.general_utils import safe_state
import uuid
from tqdm import tqdm
from utils.image_utils import psnr
from argparse import ArgumentParser, Namespace
from arguments import ModelParams, PipelineParams, OptimizationParams
from semantic_3dgs.trainer.semantic_gaussian_trainer import SemanticGaussianTrainer
from semantic_3dgs.core.adaptive_densification import SemanticAdaptiveDensification, DensificationConfig
import time
import gc
import json
import pickle
import copy

# å°è¯•å¯¼å…¥tensorboard
try:
    from torch.utils.tensorboard import SummaryWriter

    TENSORBOARD_FOUND = True
except ImportError:
    TENSORBOARD_FOUND = False

# åœºæ™¯ç‰¹å®šçš„è¯­ä¹‰é…ç½®
SCENE_SEMANTIC_CONFIGS = {
    'truck': {
        'important_labels': ["truck", "vehicle", "cargo",
        "wheel", "tire",
        "cabin", "window",
        "wood", "crate", "box",  # æœ¨è´¨è´§ç®±å¾ˆé‡è¦
        "text", "rust"],
    },

    'train': {
        'important_labels': ["train", "locomotive", "track", "railway", "signal", "wheel"],
    },

    'playground': {
        'important_labels': ["toy", "book", "text", "letter",
        "furniture", "shelf", "table",
        "mat", "rug", "cushion",
        "decoration", "wall art"],
    },

    'drjohnson': {
        'important_labels': [ "portrait", "painting", "face", "picture", "frame",
        "rug", "carpet", "pattern", "textile",
        "door", "chair", "furniture"],
    },

    # ===== Mip-NeRF 360åœºæ™¯é…ç½® =====
    'kitchen': {
        'important_labels': [
            "toy", "lego", "car", "vehicle",
            "table", "wood", "furniture",
            "mat", "rug", "fabric"
        ],
    },

    'room': {
        'important_labels': [
            "furniture", "chair", "sofa", "table",
            "speaker", "electronics",
            "bottle", "bowl", "shoe",
            "curtain", "fabric"
        ],
    },

    'bicycle': {
        'important_labels': [
            "bicycle", "bike",
            "wheel", "tire", "spoke",
            "chain", "handlebar", "seat",
            "frame", "pedal"
        ],
    },

    'garden': {
        'important_labels': [
            "plant", "flower", "tree", "grass",
            "table", "furniture", "wood",
            "pot", "decoration",
            "building", "brick", "window"
        ],
    },

    'counter': {
        'important_labels': [
            "bowl", "container", "cutting board",
            "food", "fruit", "vegetable", "onion",
            "pot", "pan", "cookware",
            "counter", "kitchen equipment"
        ],
    },

    'default': {
        'important_labels': ["face", "text", "sign", "person", "car"],
    }
}


def get_scene_config(dataset_path, force_scene_type=None):
    """æ ¹æ®æ•°æ®é›†è·¯å¾„è‡ªåŠ¨æ£€æµ‹åœºæ™¯ç±»å‹å¹¶è¿”å›é…ç½®"""
    if force_scene_type and force_scene_type in SCENE_SEMANTIC_CONFIGS:
        scene_name = force_scene_type
    else:
        scene_name = 'default'
        path_lower = dataset_path.lower()
        for scene in ['truck', 'train', 'playground', 'drjohnson', 'kitchen', 'room', 'bicycle', 'garden', 'counter']:
            if scene in path_lower:
                scene_name = scene
                break

    print(f"\n=== Scene type: '{scene_name}' ===")
    config = SCENE_SEMANTIC_CONFIGS.get(scene_name, SCENE_SEMANTIC_CONFIGS['default'])
    return scene_name, config


def compute_scene_extent_from_cameras(cameras):
    """åŸºäºç›¸æœºä½ç½®è®¡ç®—åœºæ™¯èŒƒå›´ - åŸå§‹3DGSæ–¹å¼"""
    cam_centers = []

    for cam in cameras:
        # è·å–ç›¸æœºä¸­å¿ƒï¼ˆä¸–ç•Œåæ ‡ï¼‰
        if hasattr(cam, 'camera_center'):
            cam_center = cam.camera_center
            if not isinstance(cam_center, torch.Tensor):
                cam_center = torch.tensor(cam_center)
            cam_centers.append(cam_center)
        elif hasattr(cam, 'R') and hasattr(cam, 'T'):
            R = cam.R if isinstance(cam.R, torch.Tensor) else torch.tensor(cam.R)
            T = cam.T if isinstance(cam.T, torch.Tensor) else torch.tensor(cam.T)
            cam_center = -torch.matmul(R.T, T)
            cam_centers.append(cam_center)

    if len(cam_centers) == 0:
        print("[Warning] No camera centers found, using default extent")
        return 10.0

    # è®¡ç®—ç›¸æœºçš„åŒ…å›´çƒåŠå¾„
    cam_centers = torch.stack(cam_centers)
    center = cam_centers.mean(dim=0)
    radius = torch.norm(cam_centers - center, dim=1).max().item()

    # åŸå§‹3DGSä½¿ç”¨1.1å€ä½œä¸ºå®‰å…¨è¾¹ç•Œ
    scene_extent = radius * 1.1

    print(f"[Scene Extent] Computed from {len(cameras)} cameras")
    print(f"  Camera center range: {cam_centers.min(dim=0)[0]} to {cam_centers.max(dim=0)[0]}")
    print(f"  Scene radius: {radius:.3f}")
    print(f"  Scene extent (radius * 1.1): {scene_extent:.3f}")

    return scene_extent

def load_cached_initialization(scene_type, dataset_path):
    """å°è¯•åŠ è½½ç¼“å­˜çš„è¯­ä¹‰åˆå§‹åŒ–"""
    cache_dir = "cache/semantic_init"
    metadata_file = os.path.join(cache_dir, f"{scene_type}_metadata.json")

    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)

            cache_file = metadata['cache_file']
            if os.path.exists(cache_file):
                print(f"\n=== Loading cached semantic initialization ===")
                print(f"Cache file: {cache_file}")

                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)

                # è½¬æ¢numpyå›tensor
                gaussian_params = {}
                for key, value in cache_data['gaussian_params'].items():
                    if isinstance(value, np.ndarray):
                        gaussian_params[key] = torch.from_numpy(value).cuda()
                    else:
                        gaussian_params[key] = value

                print(f"âœ“ Loaded {gaussian_params['positions'].shape[0]} cached gaussians")
                print(f"âœ“ Skip time-consuming SAM segmentation")

                spatial_lr_scale = cache_data.get('spatial_lr_scale', 1.0)
                return gaussian_params, spatial_lr_scale
            else:
                print(f"Cache file not found: {cache_file}")
        except Exception as e:
            print(f"Error loading cache: {e}")

    return None, None


def create_scaled_camera_view(original_cam, scale=1.0):
    """
    åˆ›å»ºç¼©æ”¾ç‰ˆæœ¬çš„ç›¸æœºè§†å›¾ï¼Œç”¨äºæ¸²æŸ“
    ä¸ä¿®æ”¹åŸå§‹ç›¸æœºå¯¹è±¡ï¼Œè¿”å›ä¸€ä¸ªåŒ…è£…å¯¹è±¡
    """
    if scale == 1.0:
        return original_cam

    # åˆ›å»ºä¸€ä¸ªåŠ¨æ€çš„åŒ…è£…ç±»
    class ScaledCameraWrapper:
        def __init__(self, cam, scale):
            self._original_cam = cam
            self._scale = scale

            # è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸
            self.image_width = int(cam.image_width * scale)
            self.image_height = int(cam.image_height * scale)

            # ç¼©æ”¾GTå›¾åƒ
            if hasattr(cam, 'original_image'):
                original_image = cam.original_image.unsqueeze(0)  # [1, 3, H, W]
                scaled_image = F.interpolate(
                    original_image,
                    size=(self.image_height, self.image_width),
                    mode='bilinear',
                    align_corners=False
                )
                self.original_image = scaled_image.squeeze(0)  # [3, H, W]

        def __getattr__(self, name):
            # å¯¹äºæ²¡æœ‰ç‰¹æ®Šå¤„ç†çš„å±æ€§ï¼Œç›´æ¥è¿”å›åŸå§‹ç›¸æœºçš„å±æ€§
            return getattr(self._original_cam, name)

    return ScaledCameraWrapper(original_cam, scale)


def training(dataset, opt, pipe, testing_iterations, saving_iterations,
             checkpoint_iterations, checkpoint, debug_from, scene_type=None):
    """ä¸»è®­ç»ƒå‡½æ•° - å…¨ç¨‹0.5åˆ†è¾¨ç‡ç‰ˆæœ¬"""

    # è·å–åœºæ™¯é…ç½®
    scene_name, scene_config = get_scene_config(dataset.source_path, scene_type)

    # æ£€æŸ¥é¢„å¤„ç†æ©ç 
    preprocessed_masks_dir = os.path.join(dataset.source_path, "sam_preprocessed", "masks")
    use_preprocessed_masks = os.path.exists(preprocessed_masks_dir)

    if use_preprocessed_masks:
        print(f"\nâœ“ Found preprocessed SAM masks at: {preprocessed_masks_dir}")
        print("  Will use preprocessed masks to save memory and time!")

    # åˆ›å»ºé…ç½®
    trainer_config = Namespace(
        # SAMé…ç½®
        sam_checkpoint="checkpoints/sam/sam_vit_h_4b8939.pth",
        clip_model="ViT-B/32",
        use_preprocessed_masks=use_preprocessed_masks,
        preprocessed_masks_dir=preprocessed_masks_dir if use_preprocessed_masks else None,
        use_lightweight_sam=False,

        # è¯­ä¹‰é…ç½®
        #semantic_prompts=scene_config['semantic_prompts'],
        num_init_images=5,

        # SAM-RPSé…ç½®
        grad_threshold=0.0004,
        percent_dense=0.01,
        opacity_cull=0.005,
        semantic_weight=0.3,
        #protected_labels=scene_config['protected_labels'],
        max_gaussians=1100000,

        # å†…å­˜ç®¡ç†å‚æ•°
        memory_aware=True,
        target_memory_usage=0.85,
        min_free_memory_gb=3.0,
        aggressive_pruning=False,
        pruning_min_opacity=0.005,
        pruning_max_scale=0.1,
        max_operations_per_iter=5000,

        # å…¶ä»–å‚æ•°
        opacity_reset_interval=3000,

        # SAM-ESé…ç½®
        geometric_threshold=0.15,
        semantic_iou_threshold=0.5,
        max_exploratory_points=200,
        max_holes_per_iter=50,
        #exploration_focus_areas=scene_config.get('exploration_focus', []),
        debug_mode=False
    )

    # åˆå§‹åŒ–è¯­ä¹‰è®­ç»ƒå™¨
    print("\n=== Initializing Semantic Gaussian Trainer ===")
    semantic_trainer = SemanticGaussianTrainer(trainer_config)
    if hasattr(semantic_trainer, 'initializer'):
        semantic_trainer.initializer.sh_degree = dataset.sh_degree  # ä½¿ç”¨æ•°æ®é›†çš„sh_degree

    print("Loading SAM-RPS module...")

    # åˆ›å»ºé«˜æ–¯æ¨¡å‹
    gaussians = GaussianModel(dataset.sh_degree)
    scene = Scene(dataset, gaussians)
    gaussians.training_setup(opt)

    # è®¡ç®—åœºæ™¯èŒƒå›´ - åªåšä¸€æ¬¡ï¼
    train_cameras = scene.getTrainCameras()
    scene_extent = compute_scene_extent_from_cameras(train_cameras)
    print(f"âœ“ Scene extent computed from cameras: {scene_extent:.3f}")

    # è®¾ç½®åœºæ™¯èŒƒå›´åˆ° densifier - ä½¿ç”¨æ–°çš„ set_scene_extent æ–¹æ³•
    if hasattr(semantic_trainer.densifier, 'set_scene_extent'):
        semantic_trainer.densifier.set_scene_extent(scene_extent)
        print("âœ“ Scene extent set for densifier (fixed, will not change)")

    # å¯¹ explorer ä¹Ÿè®¾ç½®å›ºå®šå€¼
    if hasattr(semantic_trainer, 'explorer'):
        semantic_trainer.explorer.scene_extent = scene_extent  # ç›´æ¥èµ‹å€¼ï¼Œä¸è°ƒç”¨æ–¹æ³•
        print("âœ“ Scene extent set for explorer")

    # åˆå§‹åŒ–æ ‡è®°
    semantic_modules_released = False

    # è¯­ä¹‰åˆå§‹åŒ–
    if scene.loaded_iter:
        print(f"Loading checkpoint from iteration {scene.loaded_iter}")
    else:
        # ========== æš‚æ—¶ç¦ç”¨è¯­ä¹‰åˆå§‹åŒ–ï¼Œä½¿ç”¨åŸå§‹3DGSæ–¹å¼ ==========
        use_semantic_init = True  # è®¾ç½®ä¸º True æ¢å¤è¯­ä¹‰åˆå§‹åŒ–

        if use_semantic_init:
            # é¦–å…ˆå°è¯•åŠ è½½ç¼“å­˜
            cached_params, cached_lr_scale = load_cached_initialization(scene_name, dataset.source_path)

            if cached_params is not None:
                # ä½¿ç”¨ç¼“å­˜çš„å‚æ•°ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
                gaussian_params = cached_params
                spatial_lr_scale = cached_lr_scale or dataset.spatial_lr_scale
                gaussians.create_from_semantic_init(gaussian_params, spatial_lr_scale)
                print("âœ“ ä»ç¼“å­˜åŠ è½½é«˜æ–¯æ¨¡å‹")
            else:
                # æ–°çš„è¯­ä¹‰å¢å¼ºåˆå§‹åŒ–ï¼ˆä½¿ç”¨å®Œæ•´ç‚¹äº‘ï¼‰
                print("\n=== ä½¿ç”¨è¯­ä¹‰å¢å¼ºåˆå§‹åŒ–ï¼ˆå®Œæ•´ç‚¹äº‘ + SAM/CLIPï¼‰===")
                print(f"åœºæ™¯ç±»å‹: {scene_name}")
                print("ç­–ç•¥: åŸå§‹3DGSåˆå§‹åŒ– + è¯­ä¹‰é‡è¦æ€§æ ‡è®°")

                # è·å–ç‚¹äº‘ï¼ˆä¸åŸå§‹3DGSåˆå§‹åŒ–ç›¸åŒçš„é€»è¾‘ï¼‰
                if hasattr(scene, 'point_cloud'):
                    point_cloud = scene.point_cloud
                    print(f"ä½¿ç”¨åœºæ™¯ç‚¹äº‘: {len(point_cloud.points)}ä¸ªç‚¹")
                else:
                    # ä»æ–‡ä»¶åŠ è½½
                    ply_path = os.path.join(dataset.source_path, "sparse/0/points3D.ply")
                    if os.path.exists(ply_path):
                        from plyfile import PlyData
                        plydata = PlyData.read(ply_path)
                        vertices = plydata['vertex']
                        positions = np.vstack([vertices['x'], vertices['y'], vertices['z']]).T
                        colors = np.vstack([vertices['red'], vertices['green'], vertices['blue']]).T / 255.0
                        normals = np.vstack([vertices['nx'], vertices['ny'], vertices['nz']]).T

                        from utils.graphics_utils import BasicPointCloud
                        point_cloud = BasicPointCloud(points=positions, colors=colors, normals=normals)
                        print(f"ä»æ–‡ä»¶åŠ è½½ç‚¹äº‘: {len(positions)}ä¸ªç‚¹")
                    else:
                        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç‚¹äº‘æ–‡ä»¶: {ply_path}")

                # å‡†å¤‡å›¾åƒå’Œç›¸æœºå‚æ•°ï¼ˆç”¨äºè¯­ä¹‰åˆ†æï¼‰
                train_images = []
                train_cameras = []
                train_image_names = []
                num_views_for_semantic = 10  # å¢åŠ åˆ°10ä¸ªè§†å›¾

                # é€‰æ‹©åˆ†å¸ƒè¾ƒå¥½çš„è§†å›¾ï¼ˆä¸è¦åªé€‰å‰Nä¸ªï¼Œè¦é€‰æ‹©ä¸åŒè§’åº¦çš„ï¼‰
                all_train_cams = scene.getTrainCameras()
                selected_indices = []

                if len(all_train_cams) <= num_views_for_semantic:
                    selected_indices = list(range(len(all_train_cams)))
                else:
                    # å‡åŒ€é‡‡æ ·ï¼Œç¡®ä¿è¦†ç›–ä¸åŒè§’åº¦
                    step = len(all_train_cams) // num_views_for_semantic
                    selected_indices = [i * step for i in range(num_views_for_semantic)]

                for idx in selected_indices:
                    cam = all_train_cams[idx]

                    # è·å–å›¾åƒ
                    gt_image = cam.original_image.permute(1, 2, 0).cpu().numpy()
                    gt_image = (gt_image * 255).astype(np.uint8)
                    train_images.append(gt_image)

                    # è·å–å›¾åƒåç§°
                    if hasattr(cam, 'image_name'):
                        train_image_names.append(cam.image_name)
                    else:
                        train_image_names.append(f"image_{idx:04d}")

                    # æ„å»ºç›¸æœºå‚æ•°å­—å…¸
                    camera_params = {}

                    # è®¡ç®—å†…å‚çŸ©é˜µ
                    import math
                    W = cam.image_width
                    H = cam.image_height

                    # ä»è§†åœºè§’è®¡ç®—ç„¦è·
                    fx = W / (2 * math.tan(cam.FoVx / 2))
                    fy = H / (2 * math.tan(cam.FoVy / 2))
                    cx = W / 2.0
                    cy = H / 2.0

                    camera_params['K'] = np.array([
                        [fx, 0, cx],
                        [0, fy, cy],
                        [0, 0, 1]
                    ], dtype=np.float32)

                    # è·å–å¤–å‚çŸ©é˜µ
                    # æ³¨æ„ï¼šworld_view_transform æ˜¯åˆ—ä¸»åºçš„ï¼Œéœ€è¦è½¬ç½®
                    W2C = cam.world_view_transform.T.cpu().numpy()
                    camera_params['R'] = W2C[:3, :3]
                    camera_params['t'] = W2C[:3, 3]

                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    if idx == selected_indices[0]:
                        print(f"  Camera {idx} intrinsics:")
                        print(f"    Image size: {W}x{H}")
                        print(f"    FoV: {math.degrees(cam.FoVx):.1f}Â° x {math.degrees(cam.FoVy):.1f}Â°")
                        print(f"    Focal length: fx={fx:.1f}, fy={fy:.1f}")
                        print(f"    Principal point: cx={cx:.1f}, cy={cy:.1f}")

                    train_cameras.append(camera_params)

                print(f"  Selected {len(selected_indices)} views from {len(all_train_cams)} total views")
                print(f"  View indices: {selected_indices}")

                # è°ƒç”¨æ–°çš„åˆå§‹åŒ–æ–¹æ³•
                # è°ƒç”¨åˆå§‹åŒ–æ–¹æ³•æ—¶
                gaussian_params = semantic_trainer.initializer.initialize_from_full_pointcloud_with_semantic(
                    train_images,
                    point_cloud,
                    train_cameras,
                    None,  # text_promptsä¸éœ€è¦äº†
                    train_image_names,
                    important_labels=scene_config.get('important_labels', ["face", "text", "sign"])
                )

                # è·å–ç›¸æœºèŒƒå›´
                spatial_lr_scale = scene.cameras_extent
                print(f"ä½¿ç”¨spatial_lr_scale (cameras_extent): {spatial_lr_scale}")

                # åˆ›å»ºé«˜æ–¯æ¨¡å‹
                gaussians.create_from_semantic_init(gaussian_params, spatial_lr_scale)

                # è®¾ç½®åœºæ™¯èŒƒå›´
                train_cameras_all = scene.getTrainCameras()
                scene_extent = compute_scene_extent_from_cameras(train_cameras_all)

                if hasattr(semantic_trainer.densifier, 'set_scene_extent'):
                    semantic_trainer.densifier.set_scene_extent(scene_extent)
                    print(f"âœ“ ä¸ºå¯†åº¦æ§åˆ¶å™¨è®¾ç½®åœºæ™¯èŒƒå›´: {scene_extent:.3f}")

                if hasattr(semantic_trainer, 'explorer'):
                    semantic_trainer.explorer.scene_extent = scene_extent
                    print(f"âœ“ ä¸ºæ¢ç´¢å™¨è®¾ç½®åœºæ™¯èŒƒå›´: {scene_extent:.3f}")

                # åˆå§‹åŒ–æ¢¯åº¦ç´¯ç§¯å™¨
                num_points = len(gaussians._xyz)
                gaussians.xyz_gradient_accum = torch.zeros((num_points, 1), device="cuda")
                gaussians.denom = torch.zeros((num_points, 1), device="cuda")
                gaussians.max_radii2D = torch.zeros(num_points, device="cuda")
                print("âœ“ æ¢¯åº¦ç´¯ç§¯å™¨å·²åˆå§‹åŒ–")

                # è®¾ç½®ä¼˜åŒ–å™¨
                gaussians.training_setup(opt)
                print("âœ“ ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

                # é‡Šæ”¾è¯­ä¹‰æ¨¡å—
                print("\n=== é‡Šæ”¾è¯­ä¹‰åˆå§‹åŒ–æ¨¡å— ===")
                if hasattr(semantic_trainer, 'initializer'):
                    del semantic_trainer.initializer
                semantic_trainer.cleanup_sam()
                gc.collect()
                torch.cuda.empty_cache()
                semantic_modules_released = True

                # æ‰“å°å†…å­˜çŠ¶æ€
                if torch.cuda.is_available():
                    free_memory, total_memory = torch.cuda.mem_get_info()
                    print(f"âœ“ æ¸…ç†åå†…å­˜: {free_memory / 1e9:.1f}GBç©ºé—² / {total_memory / 1e9:.1f}GBæ€»è®¡")

                if not use_preprocessed_masks:
                    print("\nTip: Run 'python scripts/preprocess_sam.py' to cache SAM masks for faster future runs!")
        else:
            # ========== ä½¿ç”¨åŸå§‹3DGSåˆå§‹åŒ– ==========
            print("\n=== Using Original 3DGS Initialization ===")
            print("Semantic initialization is disabled for comparison")

            # åŸå§‹3DGSä¸éœ€è¦è¯­ä¹‰è®­ç»ƒå™¨çš„åˆå§‹åŒ–æ¨¡å—
            if hasattr(semantic_trainer, 'initializer'):
                del semantic_trainer.initializer
            semantic_trainer.cleanup_sam()
            gc.collect()
            torch.cuda.empty_cache()
            semantic_modules_released = True

            # æ£€æŸ¥ç‚¹äº‘ä¿¡æ¯
            if hasattr(scene, 'point_cloud'):
                point_cloud = scene.point_cloud
                print(f"Point cloud has {len(point_cloud.points)} points")
            else:
                # ä»æ–‡ä»¶è¯»å–ç‚¹äº‘
                ply_path = os.path.join(dataset.source_path, "sparse/0/points3D.ply")
                if os.path.exists(ply_path):
                    from plyfile import PlyData
                    plydata = PlyData.read(ply_path)
                    vertices = plydata['vertex']
                    positions = np.vstack([vertices['x'], vertices['y'], vertices['z']]).T
                    colors = np.vstack([vertices['red'], vertices['green'], vertices['blue']]).T / 255.0
                    normals = np.vstack([vertices['nx'], vertices['ny'], vertices['nz']]).T

                    from utils.graphics_utils import BasicPointCloud
                    point_cloud = BasicPointCloud(points=positions, colors=colors, normals=normals)
                    print(f"Loaded point cloud with {len(positions)} points")
                else:
                    raise FileNotFoundError(f"Point cloud file not found: {ply_path}")

            # è·å–ç›¸æœºæ•°é‡ - è¿™æ˜¯ create_from_pcd éœ€è¦çš„ç¬¬äºŒä¸ªå‚æ•°
            train_cameras = scene.getTrainCameras()
            cam_infos = len(train_cameras)
            print(f"Number of training cameras: {cam_infos}")

            # ä½¿ç”¨åœºæ™¯çš„ç›¸æœºèŒƒå›´ä½œä¸º spatial_lr_scale
            spatial_lr_scale = scene.cameras_extent
            print(f"Using spatial_lr_scale (cameras_extent): {spatial_lr_scale}")

            # åŸå§‹3DGSåˆå§‹åŒ– - æ·»åŠ ç¼ºå¤±çš„ cam_infos å‚æ•°
            gaussians.create_from_pcd(point_cloud, cam_infos, spatial_lr_scale)

            if scene_name == 'truck':
                print("\n[Semantic Enhancement] Setting semantic importance for truck scene...")

                positions = gaussians._xyz.detach()  # ä½¿ç”¨detach()æ¥é¿å…æ¢¯åº¦é—®é¢˜
                num_points = positions.shape[0]

                # åˆå§‹åŒ–ä¸ºé»˜è®¤å€¼
                semantic_importance = gaussians._semantic_importance.clone()  # å·²ç»æ˜¯0.5

                # ç­–ç•¥1ï¼šåŸºäºé«˜åº¦ - å¡è½¦ä¸»ä½“é€šå¸¸åœ¨ä¸­é—´é«˜åº¦
                heights = positions[:, 1]  # Yè½´
                height_percentiles = torch.quantile(heights, torch.tensor([0.2, 0.8], device="cuda"))
                truck_body_mask = (heights > height_percentiles[0]) & (heights < height_percentiles[1])
                semantic_importance[truck_body_mask] = 0.7

                # ç­–ç•¥2ï¼šåŸºäºä½ç½®å¯†åº¦ - å¯†é›†åŒºåŸŸå¯èƒ½æ˜¯ä¸»ä½“
                from sklearn.neighbors import NearestNeighbors
                nn = NearestNeighbors(n_neighbors=20)
                nn.fit(positions.cpu().numpy())  # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è°ƒç”¨numpy()
                distances, _ = nn.kneighbors()
                density = 1.0 / (distances.mean(axis=1) + 1e-6)
                density_tensor = torch.from_numpy(density).cuda()
                density_percentile = torch.quantile(density_tensor, 0.7)
                high_density_mask = density_tensor > density_percentile
                semantic_importance[high_density_mask] = torch.maximum(
                    semantic_importance[high_density_mask],
                    torch.tensor(0.8, device="cuda")
                )

                # ç­–ç•¥3ï¼šåŸºäºåˆå§‹é¢œè‰²ï¼ˆå¦‚æœæœ‰æ·±è‰²åŒºåŸŸï¼Œå¯èƒ½æ˜¯è½®èƒï¼‰
                colors = gaussians._features_dc.detach()[:, 0, :] * 0.28209479177387814  # SH to RGB, ä¹Ÿè¦detach
                dark_mask = colors.max(dim=1)[0] < 0.3
                semantic_importance[dark_mask] = 0.9  # è½®èƒç­‰æ·±è‰²éƒ¨ä»¶ç»™æœ€é«˜é‡è¦æ€§

                # æ›´æ–°è¯­ä¹‰é‡è¦æ€§ - ç›´æ¥èµ‹å€¼ï¼Œä¸å½±å“æ¢¯åº¦
                gaussians._semantic_importance = semantic_importance

                # ç»Ÿè®¡
                high_importance = (semantic_importance > 0.7).sum().item()
                medium_importance = ((semantic_importance >= 0.5) & (semantic_importance <= 0.7)).sum().item()
                low_importance = (semantic_importance < 0.5).sum().item()

                print(f"  Total points: {num_points}")
                print(f"  High importance (>0.7): {high_importance} ({high_importance / num_points * 100:.1f}%)")
                print(
                    f"  Medium importance (0.5-0.7): {medium_importance} ({medium_importance / num_points * 100:.1f}%)")
                print(f"  Low importance (<0.5): {low_importance} ({low_importance / num_points * 100:.1f}%)")
                print(f"  Mean importance: {semantic_importance.mean().item():.3f}")
                print(f"  Max importance: {semantic_importance.max().item():.3f}")

            # ç¡®ä¿ä¼˜åŒ–å™¨è¢«åˆå§‹åŒ–
            gaussians.training_setup(opt)

            # åˆå§‹åŒ–å®Œæˆåçš„æ£€æŸ¥
            print(f"\n[Original 3DGS Init Complete]")
            print(f"  Number of gaussians: {gaussians._xyz.shape[0]}")
            print(f"  Position range: {gaussians._xyz.min(dim=0)[0]} to {gaussians._xyz.max(dim=0)[0]}")
            pos_center = gaussians._xyz.mean(dim=0)
            pos_extent = torch.norm(gaussians._xyz - pos_center, dim=1).max().item()
            print(f"  Actual extent: {pos_extent:.3f}")
            print(f"  Scene extent: {scene.cameras_extent:.3f}")
            print(f"  Ratio: {pos_extent / scene.cameras_extent:.1f}x")

            # æ‰“å°å†…å­˜çŠ¶æ€
            if torch.cuda.is_available():
                free_memory, total_memory = torch.cuda.mem_get_info()
                print(f"âœ“ Memory after init: {free_memory / 1e9:.1f}GB free / {total_memory / 1e9:.1f}GB total")

    # ç¡®ä¿ä¼˜åŒ–å™¨è¢«åˆå§‹åŒ–ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if gaussians.optimizer is None:
        gaussians.training_setup(opt)
        print("âœ“ Optimizer initialized")

    # è®¾ç½®èƒŒæ™¯
    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]
    background = torch.tensor(bg_color, dtype=torch.float32, device="cuda")

    # åˆå§‹åŒ–è¿›åº¦æ¡
    iter_start = torch.cuda.Event(enable_timing=True)
    iter_end = torch.cuda.Event(enable_timing=True)

    ema_loss_for_log = 0.0
    progress_bar = tqdm(range(opt.iterations), desc="Training progress")

    # è®­ç»ƒç»Ÿè®¡
    viewpoint_stack = None
    first_iter = 0

    # è°ƒè¯•ä¿¡æ¯é—´éš”
    debug_interval = 500
    monitor_interval = 100
    memory_check_interval = 100

    # ç»Ÿè®¡å˜é‡
    last_num_gaussians = 0
    total_splits = 0
    total_clones = 0
    total_prunes = 0

    # å†…å­˜ç®¡ç†å˜é‡
    consecutive_high_memory = 0

    # ===== æ¸²æŸ“åˆ†è¾¨ç‡æ§åˆ¶ - å…¨ç¨‹0.5 =====
    render_resolution_scale = 0.5  # å›ºå®šä½¿ç”¨0.5å€åˆ†è¾¨ç‡
    print(f"\n[Resolution] Using fixed render resolution: {render_resolution_scale}x for entire training")

    # è®°å½•åˆ†è¾¨ç‡ç»Ÿè®¡ï¼ˆç®€åŒ–ç‰ˆï¼‰
    resolution_stats = {
        'fixed_scale': render_resolution_scale,
        'memory_warnings': 0
    }

    for iteration in range(first_iter, opt.iterations + 1):
        iter_start.record()

        # === ç®€å•ç›‘æ§è¾“å‡º ===
        if iteration % monitor_interval == 0 and iteration > 0:
            num_gaussians = gaussians._xyz.shape[0]
            growth = num_gaussians - last_num_gaussians

            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            avg_opacity = torch.sigmoid(gaussians._opacity).mean().item()
            avg_scale = torch.exp(gaussians._scaling).mean().item()
            max_scale = torch.exp(gaussians._scaling).max().item()

            print(f"\n[Monitor {iteration}] Gaussians: {num_gaussians} (+{growth}), "
                  f"Loss: {ema_loss_for_log:.4f}, "
                  f"Opacity: {avg_opacity:.3f}, "
                  f"Scale: avg={avg_scale:.4f}, max={max_scale:.4f}, "
                  f"Render: {render_resolution_scale}x")

            last_num_gaussians = num_gaussians

        # === è¯¦ç»†è°ƒè¯•è¾“å‡º ===
        if iteration % debug_interval == 0 and iteration > 0:
            print(f"\n{'=' * 60}")
            print(f"[Debug Stats at iteration {iteration}]")
            print(f"{'=' * 60}")

            # åŸºæœ¬ç»Ÿè®¡
            num_gaussians = gaussians._xyz.shape[0]
            print(f"Total Gaussians: {num_gaussians}")
            print(f"Render Resolution: {render_resolution_scale}x (fixed)")

            # æ¢¯åº¦ç»Ÿè®¡
            if hasattr(gaussians, 'xyz_gradient_accum') and gaussians.denom.sum() > 0:
                # åªè®¡ç®—æœ‰æ•ˆçš„æ¢¯åº¦ï¼ˆdenom > 0ï¼‰
                valid_mask = gaussians.denom.squeeze() > 0
                if valid_mask.sum() > 0:
                    avg_grads = gaussians.xyz_gradient_accum[valid_mask] / gaussians.denom[valid_mask]
                    avg_grad = avg_grads.mean().item()
                    max_grad = avg_grads.max().item()
                    print(f"Gradient stats: avg={avg_grad:.6f}, max={max_grad:.6f}")

            # å°ºåº¦åˆ†å¸ƒ
            scales = torch.exp(gaussians._scaling)
            scale_percentiles = torch.quantile(scales.max(dim=-1)[0],
                                               torch.tensor([0.1, 0.25, 0.5, 0.75, 0.9], device="cuda"))
            print(f"Scale distribution: P10={scale_percentiles[0]:.4f}, "
                  f"P25={scale_percentiles[1]:.4f}, P50={scale_percentiles[2]:.4f}, "
                  f"P75={scale_percentiles[3]:.4f}, P90={scale_percentiles[4]:.4f}")

            # è®¡ç®—ä¼šè¢«åˆ†è£‚çš„é«˜æ–¯æ•°é‡
            if semantic_trainer.densifier.scene_extent:
                scale_threshold = 0.01 * semantic_trainer.densifier.scene_extent
                large_gaussians = (scales.max(dim=-1)[0] > scale_threshold).sum().item()
                print(f"Large gaussians (>{scale_threshold:.4f}): {large_gaussians} "
                      f"({large_gaussians / num_gaussians * 100:.1f}%)")

            # ä¸é€æ˜åº¦åˆ†å¸ƒ
            opacities = torch.sigmoid(gaussians._opacity)
            opacity_percentiles = torch.quantile(opacities.squeeze(),
                                                 torch.tensor([0.1, 0.25, 0.5, 0.75, 0.9], device="cuda"))
            print(f"Opacity distribution: P10={opacity_percentiles[0]:.3f}, "
                  f"P25={opacity_percentiles[1]:.3f}, P50={opacity_percentiles[2]:.3f}, "
                  f"P75={opacity_percentiles[3]:.3f}, P90={opacity_percentiles[4]:.3f}")

            # è¯­ä¹‰ç»Ÿè®¡
            if hasattr(gaussians, '_is_exploratory'):
                num_exploratory = gaussians._is_exploratory.sum().item()
                print(f"Exploratory gaussians: {num_exploratory} ({num_exploratory / num_gaussians * 100:.1f}%)")

            # å¯†åº¦æ§åˆ¶ç»Ÿè®¡
            stats = semantic_trainer.densifier.get_statistics()
            print(f"\nDensification stats:")
            print(f"  Total splits: {stats['total_splits']}")
            print(f"  Total clones: {stats['total_clones']}")
            print(f"  Total prunes: {stats['total_prunes']}")
            print(f"  Semantic boosts: {stats['semantic_boosts']}")

            # åˆ†è¾¨ç‡ç»Ÿè®¡
            print(f"\nResolution stats:")
            print(f"  Fixed render scale: {render_resolution_scale}x")
            print(f"  Memory warnings: {resolution_stats['memory_warnings']}")

            print(f"{'=' * 60}\n")

        # === å†…å­˜ç›‘æ§ ===
        if iteration % memory_check_interval == 0:
            if torch.cuda.is_available():
                free_memory, total_memory = torch.cuda.mem_get_info()
                used_memory = (total_memory - free_memory) / 1e9
                free_gb = free_memory / 1e9
                usage_percent = (used_memory / (total_memory / 1e9)) * 100

                # è®¡ç®—æ¸²æŸ“ç¼“å†²åŒºå¤§å°
                if 'render_cam' in locals():
                    render_buffer_size = render_cam.image_width * render_cam.image_height * 4 * 8 / 1e9
                else:
                    # ä¼°ç®—
                    original_width = scene.getTrainCameras()[0].image_width
                    original_height = scene.getTrainCameras()[0].image_height
                    render_buffer_size = (original_width * render_resolution_scale) * \
                                         (original_height * render_resolution_scale) * 4 * 8 / 1e9

                print(f"\n[Memory Check] Iteration {iteration}")
                print(f"  GPU Memory: {used_memory:.1f}GB used ({usage_percent:.1f}%), "
                      f"{free_gb:.1f}GB free")
                print(f"  Total Gaussians: {gaussians._xyz.shape[0]}")
                print(f"  Render resolution: {render_resolution_scale}x (fixed)")
                print(f"  Estimated render buffer: ~{render_buffer_size:.3f}GB")

                # å†…å­˜å‹åŠ›è­¦å‘Š
                if usage_percent > 90:
                    print(f"  âš ï¸  High memory usage! Consider reducing max_gaussians")
                    consecutive_high_memory += 1
                    resolution_stats['memory_warnings'] += 1
                elif usage_percent > 80:
                    print(f"  âš ï¸  Memory usage above 80%, monitoring closely")
                else:
                    consecutive_high_memory = 0

                # æä½å†…å­˜æ—¶çš„ç´§æ€¥æ¸…ç†
                if free_gb < 1.5:
                    print(f"  ğŸš¨ Critical memory! Forcing cleanup...")
                    torch.cuda.empty_cache()
                    gc.collect()
                    torch.cuda.synchronize()
                    time.sleep(0.1)  # ç»™GPUä¸€ç‚¹æ—¶é—´

        # å®šæœŸå†…å­˜æ¸…ç†
        if iteration % 50 == 0 and iteration > 0:
            torch.cuda.empty_cache()
            gc.collect()

        # æ›´æ–°å­¦ä¹ ç‡
        gaussians.update_learning_rate(iteration)

        # æ¯1000æ¬¡è¿­ä»£å¢åŠ SHé˜¶æ•°
        if iteration % 1000 == 0:
            gaussians.oneupSHdegree()

        # æ¯1000æ¬¡è¿­ä»£åšä¸€æ¬¡æ·±åº¦æ¸…ç†
        if iteration % 1000 == 0 and iteration > 0:
            print(f"\n[Deep Clean] Performing deep memory cleanup at iteration {iteration}")

            # åŒæ­¥æ‰€æœ‰CUDAæ“ä½œ
            torch.cuda.synchronize()

            # æ¸…ç†ä¼˜åŒ–å™¨ä¸­çš„æ­»çŠ¶æ€
            if hasattr(gaussians, 'optimizer') and gaussians.optimizer is not None:
                # è·å–å½“å‰å‚æ•°æ•°é‡
                current_size = len(gaussians._xyz)

                # æ£€æŸ¥å¹¶æ¸…ç†ä¼˜åŒ–å™¨çŠ¶æ€
                dead_states = []
                for tensor, state in gaussians.optimizer.state.items():
                    if not any(tensor is p for group in gaussians.optimizer.param_groups for p in group['params']):
                        dead_states.append(tensor)

                for tensor in dead_states:
                    del gaussians.optimizer.state[tensor]

                if dead_states:
                    print(f"  Cleaned {len(dead_states)} dead optimizer states")

            # å¤šæ¬¡å¼ºåˆ¶åƒåœ¾å›æ”¶
            for _ in range(3):
                gc.collect()
                torch.cuda.empty_cache()

            # æ‰“å°æ¸…ç†åçš„å†…å­˜çŠ¶æ€
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                free_memory, total_memory = torch.cuda.mem_get_info()
                print(
                    f"  After deep clean: {free_memory / 1e9:.1f}GB free, {(total_memory - free_memory) / 1e9:.1f}GB used")

        # éšæœºé€‰æ‹©ç›¸æœº
        if not viewpoint_stack:
            viewpoint_stack = scene.getTrainCameras().copy()
        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack) - 1))

        # ===== åˆ›å»ºç¼©æ”¾ç‰ˆæœ¬ç”¨äºæ¸²æŸ“ =====
        render_cam = create_scaled_camera_view(viewpoint_cam, render_resolution_scale)

        # æ‰“å°é¦–æ¬¡ä½¿ç”¨ç¼©æ”¾ç›¸æœºçš„ä¿¡æ¯
        if iteration == 1:
            print(f"\n[Render] First iteration render info:")
            print(f"  Original resolution: {viewpoint_cam.image_width}x{viewpoint_cam.image_height}")
            print(
                f"  Render resolution: {render_cam.image_width}x{render_cam.image_height} ({render_resolution_scale}x)")
            print(f"  Resolution fixed at {render_resolution_scale}x for entire training")

        # æ¸²æŸ“ï¼ˆä½¿ç”¨ç¼©æ”¾ç›¸æœºï¼‰
        if (iteration - 1) == debug_from:
            pipe.debug = True

        render_pkg = render(render_cam, gaussians, pipe, background)
        image, viewspace_point_tensor, visibility_filter, radii = (
            render_pkg["render"],
            render_pkg["viewspace_points"],
            render_pkg["visibility_filter"],
            render_pkg["radii"]
        )

        # è·å–ç¼©æ”¾åçš„GTå›¾åƒ
        gt_image = render_cam.original_image.cuda()

        # è®¡ç®—æŸå¤±ï¼ˆåœ¨ç¼©æ”¾åˆ†è¾¨ç‡ä¸Šï¼‰
        Ll1 = l1_loss(image, gt_image)
        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))
        loss.backward()

        # ç´¯ç§¯æ¢¯åº¦ç»Ÿè®¡ï¼ˆåœ¨å¯†åº¦æ§åˆ¶åŒºé—´å†…ï¼‰
        if iteration > opt.densify_from_iter and iteration < opt.densify_until_iter:
            # æ³¨æ„ï¼švisibility_filterå’Œradiiæ˜¯åŸºäºç¼©æ”¾åˆ†è¾¨ç‡çš„
            # ä½†æ¢¯åº¦ç´¯ç§¯ä»ç„¶æœ‰æ•ˆï¼Œå› ä¸ºæ˜¯åŸºäºå¯è§çš„é«˜æ–¯ç‚¹
            gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)

        iter_end.record()

        with torch.no_grad():
            # æ›´æ–°æŸå¤±ç»Ÿè®¡
            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log

            # è¿›åº¦æ¡æ›´æ–°
            if iteration % 10 == 0:
                progress_bar.set_postfix({
                    "Loss": f"{ema_loss_for_log:.{5}f}",
                    "Gaussians": f"{gaussians._xyz.shape[0]}",
                    "Scene": scene_name,
                    "Render": f"{render_resolution_scale}x"
                })
                progress_bar.update(10)

            # === å¯†åº¦æ§åˆ¶ï¼ˆåŸå§‹3DGSæ–¹å¼ï¼‰===
            if iteration > opt.densify_from_iter and iteration < opt.densify_until_iter:
                if iteration % 2000 == 0 and iteration > 0 and iteration < 10000:
                    # ç®€å•çš„ç§»åŠ¨è¡°å‡ï¼Œä¸éœ€è¦é‡æ–°æŠ•å½±
                    with torch.no_grad():
                        if not hasattr(gaussians, '_last_positions'):
                            gaussians._last_positions = gaussians._xyz.clone()
                        elif len(gaussians._last_positions) == len(gaussians._xyz):
                            movement = torch.norm(gaussians._xyz - gaussians._last_positions, dim=1)
                            moved_mask = movement > 0.05 * semantic_trainer.densifier.scene_extent

                            if moved_mask.sum() > 100:
                                print(f"[Semantic Decay] {moved_mask.sum()} gaussians moved significantly")
                                gaussians._semantic_importance[moved_mask] *= 0.95
                                gaussians._semantic_importance = torch.clamp(gaussians._semantic_importance, min=0.2)

                            gaussians._last_positions = gaussians._xyz.clone()
                        else:
                            # ç‚¹æ•°å˜åŒ–äº†ï¼Œé‡ç½®
                            gaussians._last_positions = gaussians._xyz.clone()
                # æ‰§è¡Œå¯†åº¦æ§åˆ¶
                if iteration % opt.densification_interval == 0:
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ - å¯†åº¦æ§åˆ¶å‰
                    print(f"\n[Debug] Before densification at iteration {iteration}:")
                    print(f"  Scene extent: {semantic_trainer.densifier.scene_extent:.3f}")

                    # æ£€æŸ¥é«˜æ–¯ä½ç½®èŒƒå›´
                    pos_min = gaussians._xyz.min(dim=0)[0]
                    pos_max = gaussians._xyz.max(dim=0)[0]
                    actual_extent = (pos_max - pos_min).max().item()
                    print(f"  Gaussian positions range: {actual_extent:.3f}")
                    print(f"  Position bounds - Min: [{pos_min[0]:.2f}, {pos_min[1]:.2f}, {pos_min[2]:.2f}], "
                          f"Max: [{pos_max[0]:.2f}, {pos_max[1]:.2f}, {pos_max[2]:.2f}]")

                    # å…ˆç®¡ç†æ¢ç´¢æ€§é«˜æ–¯çš„ç”Ÿå‘½å‘¨æœŸï¼ˆå¦‚æœæœ‰ï¼‰
                    if hasattr(semantic_trainer, 'lifecycle_manager') and hasattr(gaussians, '_is_exploratory'):
                        lifecycle_stats = semantic_trainer.lifecycle_manager.manage_exploratory_lifecycle(
                            gaussians, iteration, opt.iterations
                        )
                        if iteration % 1000 == 0 and lifecycle_stats['promoted'] > 0:
                            print(
                                f"[Lifecycle] Promoted {lifecycle_stats['promoted']} exploratory gaussians to permanent")

                    # è®¾ç½®å½“å‰è¿­ä»£ï¼ˆç”¨äºå‰ªæä¿æŠ¤ï¼‰
                    semantic_trainer.densifier.current_iteration = iteration
                    semantic_trainer.densifier.max_iterations = opt.iterations

                    # è®¾ç½®å±å¹•ç©ºé—´å‰ªæé˜ˆå€¼ï¼ˆåŸå§‹3DGSçš„é€»è¾‘ï¼‰
                    if iteration > opt.opacity_reset_interval:  # 3000è¿­ä»£å
                        size_threshold = 40  # å±å¹•ç©ºé—´20åƒç´ 
                    else:
                        size_threshold = None  # å‰3000è¿­ä»£ä¸ç”¨

                    # è·å–å½“å‰å‚æ•°
                    gaussians_dict = gaussians.get_params_dict()

                    # è®°å½•æ“ä½œå‰çš„æ•°é‡
                    num_before = len(gaussians_dict['positions'])

                    # é€šè¿‡traineræ‰§è¡Œå¯†åº¦æ§åˆ¶
                    updated_dict = semantic_trainer.densify_and_prune(
                        gaussians_dict,
                        viewspace_point_tensor,
                        visibility_filter,
                        radii,
                        iteration,
                        semantic_trainer.densifier.scene_extent,
                        max_screen_size=size_threshold
                    )

                    # æ›´æ–°é«˜æ–¯æ¨¡å‹
                    gaussians.update_from_params_dict(updated_dict)

                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ - å¯†åº¦æ§åˆ¶å
                    print(f"\n[Debug] After densification at iteration {iteration}:")
                    print(f"  Scene extent: {semantic_trainer.densifier.scene_extent:.3f}")

                    # å†æ¬¡æ£€æŸ¥é«˜æ–¯ä½ç½®èŒƒå›´
                    pos_min_after = gaussians._xyz.min(dim=0)[0]
                    pos_max_after = gaussians._xyz.max(dim=0)[0]
                    actual_extent_after = (pos_max_after - pos_min_after).max().item()
                    print(f"  Gaussian positions range: {actual_extent_after:.3f}")
                    print(
                        f"  Position bounds - Min: [{pos_min_after[0]:.2f}, {pos_min_after[1]:.2f}, {pos_min_after[2]:.2f}], "
                        f"Max: [{pos_max_after[0]:.2f}, {pos_max_after[1]:.2f}, {pos_max_after[2]:.2f}]")

                    # æ£€æŸ¥åœºæ™¯èŒƒå›´æ˜¯å¦è¢«æ„å¤–ä¿®æ”¹
                    if hasattr(semantic_trainer.densifier, '_original_scene_extent'):
                        if abs(semantic_trainer.densifier.scene_extent - semantic_trainer.densifier._original_scene_extent) > 0.01:
                            print(f"  âš ï¸ WARNING: Scene extent changed! "
                                  f"Original: {semantic_trainer.densifier._original_scene_extent:.3f}, "
                                  f"Current: {semantic_trainer.densifier.scene_extent:.3f}")

                    # è¾“å‡ºç»Ÿè®¡
                    num_after = len(gaussians._xyz)
                    stats = semantic_trainer.densifier.get_statistics()
                    print(
                        f"\n[Densify {iteration}] {num_before} -> {num_after} gaussians (render scale: {render_resolution_scale}x)")
                    print(
                        f"  Operations: {stats['total_splits']} splits, {stats['total_clones']} clones, {stats['total_prunes']} prunes")
                    if stats['semantic_boosts'] > 0:
                        print(f"  Semantic boosts: {stats['semantic_boosts']}")

                # æ¯1000æ¬¡è¿­ä»£é¢å¤–æ£€æŸ¥ä¸€æ¬¡
                elif iteration % 1000 == 0:
                    print(f"\n[Debug Check at iteration {iteration}]")
                    print(f"  Scene extent: {semantic_trainer.densifier.scene_extent:.3f}")

                    pos_min = gaussians._xyz.min(dim=0)[0]
                    pos_max = gaussians._xyz.max(dim=0)[0]
                    actual_extent = (pos_max - pos_min).max().item()
                    print(f"  Gaussian positions range: {actual_extent:.3f}")
                    print(f"  Position bounds - Min: [{pos_min[0]:.2f}, {pos_min[1]:.2f}, {pos_min[2]:.2f}], "
                          f"Max: [{pos_max[0]:.2f}, {pos_max[1]:.2f}, {pos_max[2]:.2f}]")

                    # å¦‚æœä½ç½®èŒƒå›´è¿œå¤§äºåœºæ™¯èŒƒå›´ï¼Œå‘å‡ºè­¦å‘Š
                    if actual_extent > semantic_trainer.densifier.scene_extent * 2:
                        print(f"  âš ï¸ WARNING: Gaussian positions ({actual_extent:.3f}) "
                              f"far exceed scene extent ({semantic_trainer.densifier.scene_extent:.3f})!")

                # === æ¢ç´¢æ€§åˆ†è£‚ï¼ˆä½¿ç”¨å…¨åˆ†è¾¨ç‡ï¼‰===
                if iteration > 500 and iteration % 500 == 0:
                    current_num_gaussians = gaussians._xyz.shape[0]

                    # å½“é«˜æ–¯æ•°é‡è¶…è¿‡10ä¸‡æ—¶ï¼Œå®Œå…¨ç¦ç”¨æ¢ç´¢æ€§åˆ†è£‚
                    if current_num_gaussians > 100000:
                        print(f"[ES] Disabled due to high gaussian count: {current_num_gaussians} > 100k")
                    elif torch.cuda.is_available():
                        free_memory, _ = torch.cuda.mem_get_info()
                        if free_memory > 3e9:  # æœ‰è¶³å¤Ÿå†…å­˜
                            # å¯¹äºæ¢ç´¢æ€§åˆ†è£‚ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸå§‹åˆ†è¾¨ç‡ä»¥ä¿è¯ç²¾åº¦
                            print(f"\n[ES] Performing exploratory split at iteration {iteration}")

                            # å§‹ç»ˆéœ€è¦é‡æ–°æ¸²æŸ“å…¨åˆ†è¾¨ç‡ï¼Œå› ä¸ºæˆ‘ä»¬å…¨ç¨‹ç”¨0.5x
                            print(
                                f"  Re-rendering at full resolution for hole detection (training uses {render_resolution_scale}x)")
                            full_render_pkg = render(viewpoint_cam, gaussians, pipe, background)
                            full_image = full_render_pkg["render"]
                            full_gt_image = viewpoint_cam.original_image.cuda()

                            # å‡†å¤‡ç›¸æœºå‚æ•°
                            camera_params = {}

                            # å¤„ç†KçŸ©é˜µ
                            if hasattr(viewpoint_cam, 'K'):
                                K = viewpoint_cam.K
                                camera_params['K'] = K if isinstance(K, np.ndarray) else K.cpu().numpy()
                            else:
                                camera_params['K'] = np.eye(3)

                            # å¤„ç†RçŸ©é˜µ
                            if hasattr(viewpoint_cam, 'R'):
                                R = viewpoint_cam.R
                                camera_params['R'] = R if isinstance(R, np.ndarray) else R.cpu().numpy()
                            else:
                                camera_params['R'] = np.eye(3)

                            # å¤„ç†tå‘é‡
                            if hasattr(viewpoint_cam, 'T'):
                                t = viewpoint_cam.T
                                camera_params['t'] = t if isinstance(t, np.ndarray) else t.cpu().numpy()
                            else:
                                camera_params['t'] = np.zeros(3)

                            image_name = viewpoint_cam.image_name if hasattr(viewpoint_cam, 'image_name') else None

                            # æ‰§è¡Œæ¢ç´¢æ€§åˆ†è£‚
                            num_created = semantic_trainer.explorer.detect_and_create_exploratory_gaussians(
                                full_image.permute(1, 2, 0),
                                full_gt_image.permute(1, 2, 0),
                                gaussians,
                                camera_params,
                                iteration,
                                image_name
                            )

                            if num_created > 0:
                                print(f"[ES] Created {num_created} exploratory gaussians")
                        else:
                            print(f"[ES] Skipped due to low memory: {free_memory / 1e9:.1f}GB")

                # é‡ç½®ä¸é€æ˜åº¦
                if iteration % opt.opacity_reset_interval == 0 and iteration < opt.densify_until_iter:
                    print(f"\n[Reset] Resetting opacity at iteration {iteration} (during densification)")
                    gaussians.reset_opacity()
                    # å¼ºåˆ¶åŒæ­¥å’Œæ¸…ç†
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    gc.collect()

                    # ç­‰å¾…ä¸€ä¸‹è®©GPUç¨³å®š
                    time.sleep(0.5)

                    # å†æ¬¡æ£€æŸ¥å†…å­˜
                    free_memory, total_memory = torch.cuda.mem_get_info()
                    print(f"[Reset] After cleanup: {free_memory / 1e9:.1f}GB free")

            # ä¼˜åŒ–å™¨æ­¥è¿›
            if iteration < opt.iterations:
                # æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                # å¯¹ä½ç½®å‚æ•°è¿›è¡Œæ¢¯åº¦è£å‰ª
                if gaussians._xyz.grad is not None:
                    torch.nn.utils.clip_grad_norm_([gaussians._xyz], max_norm=1.0)

                # å¯¹ä¸é€æ˜åº¦å‚æ•°è¿›è¡Œæ¢¯åº¦è£å‰ªï¼ˆé™ä½é˜ˆå€¼ï¼‰
                if gaussians._opacity.grad is not None:
                    torch.nn.utils.clip_grad_norm_([gaussians._opacity], max_norm=1.0)  # æ›´ä¸¥æ ¼

                # å¯¹ç¼©æ”¾å‚æ•°è¿›è¡Œæ¢¯åº¦è£å‰ª
                if gaussians._scaling.grad is not None:
                    torch.nn.utils.clip_grad_norm_([gaussians._scaling], max_norm=1.0)

                # å¯¹æ—‹è½¬å‚æ•°è¿›è¡Œæ¢¯åº¦è£å‰ª
                if gaussians._rotation.grad is not None:
                    torch.nn.utils.clip_grad_norm_([gaussians._rotation], max_norm=1.0)

                # å¯¹ç‰¹å¾å‚æ•°è¿›è¡Œæ¢¯åº¦è£å‰ª
                if gaussians._features_dc.grad is not None:
                    torch.nn.utils.clip_grad_norm_([gaussians._features_dc], max_norm=1.0)

                if gaussians._features_rest.grad is not None:
                    torch.nn.utils.clip_grad_norm_([gaussians._features_rest], max_norm=1.0)

                # æ‰§è¡Œä¼˜åŒ–å™¨æ­¥è¿›
                gaussians.optimizer.step()

                # æ¸…é›¶æ¢¯åº¦
                gaussians.optimizer.zero_grad(set_to_none=True)

                # æ¯100æ¬¡è¿­ä»£è¾“å‡ºä¸€æ¬¡æ¢¯åº¦ä¿¡æ¯ç”¨äºè°ƒè¯•
                if iteration % 100 == 0 and iteration > 0:
                    gaussians.compress_optimizer_state()
                    grad_norms = []
                    if gaussians._xyz.grad is not None:
                        grad_norms.append(('xyz', torch.norm(gaussians._xyz.grad).item()))
                    if gaussians._opacity.grad is not None:
                        grad_norms.append(('opacity', torch.norm(gaussians._opacity.grad).item()))
                    if gaussians._scaling.grad is not None:
                        grad_norms.append(('scaling', torch.norm(gaussians._scaling.grad).item()))

                    if grad_norms:
                        print(f"[Grad Norms {iteration}] " + ", ".join(
                            [f"{name}: {norm:.4f}" for name, norm in grad_norms]))

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if iteration in checkpoint_iterations:
                print(f"\n[ITER {iteration}] Saving Checkpoint")
                torch.save((gaussians.capture(), iteration),
                           scene.model_path + "/chkpnt" + str(iteration) + ".pth")

            # ä¿å­˜åœºæ™¯
            if iteration in saving_iterations:
                print(f"\n[ITER {iteration}] Saving Gaussians")
                scene.save(iteration)

                # æœ€ç»ˆç»Ÿè®¡
                print(f"\nStatistics at save point (iteration {iteration}) for scene '{scene_name}':")
                print(f"  Total Gaussians: {gaussians._xyz.shape[0]}")
                print(f"  Render resolution: {render_resolution_scale}x (fixed)")
                final_stats = semantic_trainer.densifier.get_statistics()
                print(f"  Total operations: {final_stats['total_splits']} splits, "
                      f"{final_stats['total_clones']} clones, "
                      f"{final_stats['total_prunes']} prunes")
                print(f"  Current loss: {ema_loss_for_log:.4f}")
                print(f"  Memory warnings during training: {resolution_stats['memory_warnings']}")

    print("\n=== Training Complete ===")

    # è·å–æœ€ç»ˆç»Ÿè®¡
    final_stats = semantic_trainer.get_training_stats(gaussians.get_params_dict(), iteration)
    print(f"\nFinal statistics for scene '{scene_name}':")
    print(f"  Total Gaussians: {final_stats['num_gaussians']}")
    print(f"  Exploratory Gaussians: {final_stats.get('num_exploratory', 0)}")

    # æ‰“å°æœ€ç»ˆå†…å­˜ä¿¡æ¯
    if 'memory_info' in final_stats:
        mem = final_stats['memory_info']
        print(f"  Final GPU Memory: {mem['gpu_used_gb']:.1f}GB used ({mem['gpu_usage_percent']:.1f}%), "
              f"{mem['gpu_free_gb']:.1f}GB free")

    # æ‰“å°åˆ†è¾¨ç‡ç»Ÿè®¡æ€»ç»“
    print(f"\nRender resolution summary:")
    print(f"  Fixed resolution throughout training: {render_resolution_scale}x")
    print(f"  Total memory warnings: {resolution_stats['memory_warnings']}")
    print(f"\nNote: Training used {render_resolution_scale}x resolution for memory efficiency.")
    print(f"      Test/evaluation should use full resolution for best quality.")


def prepare_output_and_logger(args):
    """å‡†å¤‡è¾“å‡ºç›®å½•å’Œæ—¥å¿—"""
    if not args.model_path:
        if os.getenv('OAR_JOB_ID'):
            unique_str = os.getenv('OAR_JOB_ID')
        else:
            unique_str = str(uuid.uuid4())
        args.model_path = os.path.join("./output/", unique_str[0:10])

    print(f"Output folder: {args.model_path}")
    os.makedirs(args.model_path, exist_ok=True)

    with open(os.path.join(args.model_path, "cfg_args"), 'w') as cfg_log_f:
        cfg_log_f.write(str(Namespace(**vars(args))))

    tb_writer = None
    if TENSORBOARD_FOUND:
        tb_writer = SummaryWriter(args.model_path)
    else:
        print("Tensorboard not available: not logging progress")

    return tb_writer


if __name__ == "__main__":
    # è®¾ç½®å‚æ•°
    parser = ArgumentParser(description="Training script parameters")
    lp = ModelParams(parser)
    op = OptimizationParams(parser)
    pp = PipelineParams(parser)
    parser.add_argument('--ip', type=str, default="127.0.0.1")
    parser.add_argument('--port', type=int, default=6009)
    parser.add_argument('--debug_from', type=int, default=-1)
    parser.add_argument('--detect_anomaly', action='store_true', default=False)
    parser.add_argument("--test_iterations", nargs="+", type=int, default=[7_000, 30_000])
    parser.add_argument("--save_iterations", nargs="+", type=int, default=[7_000, 30_000])
    parser.add_argument("--quiet", action="store_true")
    parser.add_argument("--checkpoint_iterations", nargs="+", type=int, default=[])
    parser.add_argument("--start_checkpoint", type=str, default=None)
    parser.add_argument("--scene_type", type=str, default=None,
                        choices=['truck', 'train', 'playground', 'drjohnson',
                                 'kitchen', 'room', 'bicycle', 'garden', 'counter', 'default'],
                        help="Manually specify scene type for semantic configuration")
    parser.add_argument("--use_preprocessed_masks", action="store_true", default=True,
                        help="Use preprocessed SAM masks if available")

    args = parser.parse_args(sys.argv[1:])
    args.save_iterations.append(args.iterations)

    print("Optimizing " + args.model_path)

    # å®‰å…¨éšæœºç§å­
    safe_state(args.quiet)

    # Start GUI server, configure and run training
    network_gui.init(args.ip, args.port)
    torch.autograd.set_detect_anomaly(args.detect_anomaly)

    # è®­ç»ƒ
    training(lp.extract(args), op.extract(args), pp.extract(args),
             args.test_iterations, args.save_iterations, args.checkpoint_iterations,
             args.start_checkpoint, args.debug_from, scene_type=args.scene_type)

    print("\nTraining complete.")

